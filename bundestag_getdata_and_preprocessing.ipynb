{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bundestag_getdata-and-preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQgKGiy/Xmlo31qmriPREx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WittJonas/bundestag/blob/main/bundestag_getdata_and_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV889H7Cc1zI"
      },
      "source": [
        "import tweepy\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re   \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "plt.style.use('fivethirtyeight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH6oUMPdxkvz"
      },
      "source": [
        "consumerKey = '***'\n",
        "consumerSecret = '***'\n",
        "accessToken = '***'\n",
        "accessTokenSecret = '***'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDJrkScQzhCu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf441261-da9b-42b0-fea4-1f7d5c2a44d4"
      },
      "source": [
        "# authenticate to Twitter\n",
        "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
        "auth.set_access_token(accessToken, accessTokenSecret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# test authentication\n",
        "try:\n",
        "    api.verify_credentials()\n",
        "    print(\"Authentication OK\")\n",
        "except:\n",
        "    print(\"Error during authentication\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authentication OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKs4tIZR4UTf"
      },
      "source": [
        "pull_linke = []\n",
        "for tweet in tweepy.Cursor(api.user_timeline, screen_name =\"Linksfraktion\", tweet_mode = \"extended\").items(20000):\n",
        "  pull_linke.append(tweet)\n",
        "\n",
        "pull_gruene = []\n",
        "for tweet in tweepy.Cursor(api.user_timeline, screen_name =\"GrueneBundestag\", tweet_mode = \"extended\").items(20000):\n",
        "  pull_gruene.append(tweet)\n",
        "\n",
        "pull_spd = []\n",
        "for tweet in tweepy.Cursor(api.user_timeline, screen_name =\"spdbt\", tweet_mode = \"extended\").items(20000):\n",
        "  pull_spd.append(tweet)\n",
        "\n",
        "pull_cdu = []\n",
        "for tweet in tweepy.Cursor(api.user_timeline, screen_name =\"cducsubt\", tweet_mode = \"extended\").items(20000):\n",
        "  pull_cdu.append(tweet)\n",
        "\n",
        "pull_fdp = []\n",
        "for tweet in tweepy.Cursor(api.user_timeline, screen_name =\"fdpbt\", tweet_mode = \"extended\").items(20000):\n",
        "  pull_fdp.append(tweet)\n",
        "\n",
        "pull_afd = []\n",
        "for tweet in tweepy.Cursor(api.user_timeline, screen_name =\"AfDimBundestag\", tweet_mode = \"extended\").items(20000):\n",
        "  pull_afd.append(tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOELRwSa4ier"
      },
      "source": [
        "for tweet in pull_linke[0:10]:\n",
        "  print(tweet.full_text + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZe-YYiN5nUf"
      },
      "source": [
        "# create dataframes\n",
        "linke = pd.DataFrame([tweet.full_text for tweet in pull_linke], columns=['Tweets'])\n",
        "gruene = pd.DataFrame([tweet.full_text for tweet in pull_gruene], columns=['Tweets'])\n",
        "spd = pd.DataFrame([tweet.full_text for tweet in pull_spd], columns=['Tweets'])\n",
        "cdu = pd.DataFrame([tweet.full_text for tweet in pull_cdu], columns=['Tweets'])\n",
        "fdp = pd.DataFrame([tweet.full_text for tweet in pull_fdp], columns=['Tweets'])\n",
        "afd = pd.DataFrame([tweet.full_text for tweet in pull_afd], columns=['Tweets'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0G6wtVs59ra"
      },
      "source": [
        "# exclude retweets and reindex dataframe\n",
        "\n",
        "parties=[linke, gruene, spd, cdu, fdp, afd]\n",
        "\n",
        "for df in parties:\n",
        "  df = df[~df.Tweets.str.startswith('RT')] # exclude retweets\n",
        "  df = df.reset_index(drop=True) # reindex dataframe\n",
        "  print(df.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL5GUNAZSSCg",
        "outputId": "de8ed7ae-73f3-4e96-d7b3-2b9b3b78d881"
      },
      "source": [
        "cdu.Tweets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Heute vor 60 Jahren begann der Mauerbau. FÃ¼r e...\n",
              "1       ðŸ”µPresseinfoðŸ‘‰_connemanEinsatz fÃ¼r SED-Opfer dar...\n",
              "2       Eine wichtige Errungenschaft der vergangenen 4...\n",
              "3       2019 haben wir als Fraktion mit dem LeipzigerA...\n",
              "4       Morgen jÃ¤hrt sich der Beginn des Mauerbau|s in...\n",
              "                              ...                        \n",
              "1664    Geschickt in Infrastrukturprojekte investieren...\n",
              "1665    ðŸ”µ Pressemitteilung ðŸ‘‰ Heute ist der Internation...\n",
              "1666    Alfred Dregger war eine der prÃ¤genden PersÃ¶nli...\n",
              "1667    ðŸ”µPressemitteilung â–¶ Die Koalitionsfraktionen h...\n",
              "1668    12 Milliarden Euro fÃ¼r Familien, Senioren oder...\n",
              "Name: Tweets, Length: 1669, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00Zdm-IMHgSB"
      },
      "source": [
        "# # define function to clean the text\n",
        "# def cleanTxt(text):\n",
        "#   text = re.sub('@[A-Za-z0-9]+', '', text) #remove @mentions\n",
        "#   text = re.sub(r'#', '', text) #remove '#' symbol\n",
        "#   text = re.sub(r'â€ž', '', text) #remove 'â€ž' symbol\n",
        "#   text = re.sub(r'â€œ', '', text) #remove 'â€œ' symbol\n",
        "#   # text = re.sub(r'+++', '', text) #remove '+++' symbol\n",
        "#   text = re.sub(r'&amp', '', text) #remove '&' symbol\n",
        "#   text = re.sub(r'.: ', '', text) #remove '.: '\n",
        "#   text = re.sub(r'ðŸ”µ ', '', text) #remove 'ðŸ”µ'\n",
        "#   text = re.sub(r'ðŸ‘‰', '', text) #remove 'ðŸ‘‰'\n",
        "#   text = re.sub(r'https?:\\/\\/\\S+', '', text) #remove hyper link\n",
        "#   text = re.sub(r'\\n', ' ', text) #remove '/n'\n",
        "  \n",
        "#   return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoUBC0vKV6zi"
      },
      "source": [
        "# define function to clean the text\n",
        "\n",
        "def cleanTxt(text):\n",
        "\n",
        "  # remove all kinds of emojies \n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "  emoji_pattern.sub(r'', text)\n",
        "\n",
        "  # list of symbols / expressions / words that occur frequently in tweets that should be deleted\n",
        "  removal_list = ['Pressemitteilung', 'PRESSESTATEMENT', 'PRESSEMITTEILUNG', 'Presseinfo', '@[A-Za-z0-9]+', '@_[A-Za-z0-9]+', \n",
        "                  '#', 'â€ž', 'â€œ', '&amp', '.: ', 'https?:\\/\\/\\S+', '\\n', '_', ' , , ,']\n",
        "  for i in removal_list:\n",
        "    text = text = re.sub(i, '', text)\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA1ZZWrM9XU5"
      },
      "source": [
        "# apply function to parties\n",
        "\n",
        "parties=[linke, gruene, spd, cdu, fdp, afd]\n",
        "\n",
        "for df in parties:\n",
        "  df.Tweets = df.Tweets.apply(cleanTxt)\n",
        "  print(df.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW0K1UsKxRrk"
      },
      "source": [
        "# replace empty strings with NA and drop NA\n",
        "linke = linke.replace(r'^\\s*$', np.nan, regex=True)\n",
        "linke = linke.dropna()\n",
        "gruene = gruene.replace(r'^\\s*$', np.nan, regex=True)\n",
        "gruene = gruene.dropna()\n",
        "spd = spd.replace(r'^\\s*$', np.nan, regex=True)\n",
        "spd = spd.dropna()\n",
        "cdu = cdu.replace(r'^\\s*$', np.nan, regex=True)\n",
        "cdu = cdu.dropna()\n",
        "fdp = fdp.replace(r'^\\s*$', np.nan, regex=True)\n",
        "fdp = fdp.dropna()\n",
        "afd = afd.replace(r'^\\s*$', np.nan, regex=True)\n",
        "afd = afd.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB6WBZvkzewX"
      },
      "source": [
        "# replace empty strings with tweet length < 20 char\n",
        "for df in parties:\n",
        "  index_names = df[df.Tweets.str.len() < 20].index # get indices of short tweets\n",
        "  df.drop(index_names, inplace = True) # drop rows\n",
        "  # print(df.info())\n",
        "\n",
        "# Reindex dataframes\n",
        "linke = linke.reset_index(drop=True)\n",
        "gruene = gruene.reset_index(drop=True)\n",
        "spd = spd.reset_index(drop=True)\n",
        "cdu = cdu.reset_index(drop=True)\n",
        "fdp = fdp.reset_index(drop=True)\n",
        "afd = afd.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESYGtmEBAkCs"
      },
      "source": [
        "# Combine to one dataframe and generate label column for later training\n",
        "linke['label'] = 0 \n",
        "gruene['label'] = 1\n",
        "spd['label'] = 2 \n",
        "cdu['label'] = 3\n",
        "fdp['label'] = 4 \n",
        "afd['label'] = 5\n",
        "\n",
        "frames = [linke, gruene, spd, cdu, fdp, afd]\n",
        "data = pd.concat(frames)\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# Reindex dataframes\n",
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH1EwLlZn4wX"
      },
      "source": [
        "# mount drive to save data\n",
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dQrEj2p7Ig0"
      },
      "source": [
        "# save data individually\n",
        "linke.to_csv('linke_origin_b.csv', index=False)\n",
        "!cp linke_origin_b.csv \"drive/My Drive/NLP_project\"\n",
        "\n",
        "gruene.to_csv('gruene_origin_b.csv', index=False)\n",
        "!cp gruene_origin_b.csv \"drive/My Drive/NLP_project\"\n",
        "\n",
        "spd.to_csv('spd_origin_b.csv', index=False)\n",
        "!cp spd_origin_b.csv \"drive/My Drive/NLP_project\"\n",
        "\n",
        "cdu.to_csv('cdu_origin_b.csv', index=False)\n",
        "!cp cdu_origin_b.csv \"drive/My Drive/NLP_project\"\n",
        "\n",
        "fdp.to_csv('fdp_origin_b.csv', index=False)\n",
        "!cp fdp_origin_b.csv \"drive/My Drive/NLP_project\"\n",
        "\n",
        "afd.to_csv('afd_origin_b.csv', index=False)\n",
        "!cp afd_origin_b.csv \"drive/My Drive/NLP_project\"\n",
        "\n",
        "#save data concatenated\n",
        "test.to_csv('test.csv', index=False)\n",
        "!cp test.csv \"drive/My Drive/NLP_project\"\n",
        "\n",
        "train.to_csv('train.csv', index=False)\n",
        "!cp train.csv \"drive/My Drive/NLP_project\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIQ1HJpPhzTA"
      },
      "source": [
        "# load data\n",
        "linke = pd.read_csv(\"drive/My Drive/NLP_project/linke_origin_b.csv\")\n",
        "gruene = pd.read_csv(\"drive/My Drive/NLP_project/gruene_origin_b.csv\")\n",
        "spd = pd.read_csv(\"drive/My Drive/NLP_project/spd_origin_b.csv\")\n",
        "cdu = pd.read_csv(\"drive/My Drive/NLP_project/cdu_origin_b.csv\")\n",
        "fdp = pd.read_csv(\"drive/My Drive/NLP_project/fdp_origin_b.csv\")\n",
        "afd = pd.read_csv(\"drive/My Drive/NLP_project/afd_origin_b.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}